<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Jcoz by Decave</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-dark.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="stylesheets/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>
      <div id="header">
        <nav>
          <li class="fork"><a href="https://github.com/Decave/JCoz">View On GitHub</a></li>
          <li class="downloads"><a href="https://github.com/Decave/JCoz/zipball/master">ZIP</a></li>
          <li class="downloads"><a href="https://github.com/Decave/JCoz/tarball/master">TAR</a></li>
          <li class="title">DOWNLOADS</li>
        </nav>
      </div><!-- end header -->

    <div class="wrapper">

      <section>
        <div id="title">
          <h1>Jcoz</h1>
          <p>Java Causal Profiler</p>
          <p>By Matthew Perron (mperron) and David Vernet (dcv)</p>
          <hr>
          <span class="credits left">&copy; Copyright 2016 <a href="https://github.com/Decave">David Vernet</a> and <a href="https://github.com/MattPerron">Matthew Perron</a> all rights reserved</span>
          <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
        </div>

    <h2>Summary</h2>
          <p>
              We implemented a causal Java profiler, and were able to use it to optimize the <a href="http://www.h2database.com/html/main.html">Java H2 Database</a>; a widely used, mature in-memory Java database engine.
              Our profiler is also very lightweight, with roughly 10-30% runtime overhead (beating major profilers such as hprof, JRat, and YourKit).
          </p>

    <h2>Background</h2>
          <p>
              Optimizing parallel programs is hard...very hard. As we've learned in this class, there are many moving parts to consider. For example, if threads are write sharing a global variable
              it can cause a ton of interconnect traffic and cache misses. Or perhaps Ahmdal's law means that we'll never reach our performance goal before we even get started, and we end up
              wasting time optimizing a program that will never see any performance benefit no matter how hard we try. Even worse,
              in a parallel program, speeding up certain parts of your code can sometimes even cause a performance <i>hit</i> (for example, when speeding up a line of code causes increased
              lock contention). Is it possible to handle all of this complexity in a profiler, and provide accurate results so programmers know exactly where to look to optimize
              their parallel programs? That's where causal profiling comes in.
          </p>
          <p>
              For our project, we implemented a causal Java profiler; that is, a tool for profiling multithreaded Java programs. A causal profiler detects how changing a line would
              affect performance using something called <i>virtual speedup</i>. At some frequency throughout the runtime of the program, we run an "experiment" in which we
              choose a line being executed among all threads (randomly), and a speedup amount
              between 0 - 100%, to measure how speeding up that line of code by the given speedup percent would affect overall program runtime.
              During this experiment, when any thread enters the selected line, all other threads are suspended for a period of time depending
              on the speedup chosen for that experiment. We measure the throughput achieved during the experiment, and use it
              to determine how speeding up the line would affect throughput / runtime. Thus, by freezing the other threads and measuring how throughput changes, we have "virtually" sped
              the line being executed by the given thread.
          </p>
          <p>
              The following caption is a useful visualization of virtual speedup (caption credited to Charlie Curtsinger and Emery Burger whose paper is cited in the references section): <br /> <br />
              <img src="http://davidvernet.com/media/virtual_speedup_picture.png" />
          </p>
          <p>
              The above approach to profiling multithreaded programs has many benefits over the traditional performance monitoring profilers.
              For example, observe the following toy program:
              <tt>
                  <br /><br />void a() {<br />
                  &emsp;for(volatile size_t x = 0; x < 12000000000; x++) {} <br />
                  }<br />
                  void b() {<br />
                  &emsp;for(volatile size_t x = 0; x < 8000000000; x++) {}<br />
                  }<br />
                  <br />
                  int main() {<br />
                  &emsp;thread thread_a(a), thread_b(b);<br />
                  &emsp;thread_a.join(); thread_b.join();<br />
                  }<br />
              </tt><br />
              For this program, an instrumentation profiler, hprof, gives us the following output:
              <tt>
                  <br /><br />
                  rank   self  accum   count trace method<br />
                  1 37.50% 37.50%       2 301435 java.lang.Thread.join // join statements<br />
                  2 37.50% 75.00%       1 301434 test.Test$ThreadTest1.run // a()<br />
                  3 24.96% 99.96%       1 301404 test.Test$ThreadTest2.run // b()<br /><br />
              </tt>

              As you can see, hprof tells us that we spend over 1/3 of our time in a join statement and in <tt>a()</tt>,
              and 1/4 of our time in <tt>b()</tt> In reality, optimizing <tt>b()</tt> would have no affect whatsoever on program runtime,
              and optimizing the join statement is meaningless. We see similar results when running hprof's sampling profiler:

              <tt>
                  <br /><br />
                  rank   self  accum   count trace method<br />
                  1 59.87% 59.87%    4836 300032 test.Test$ThreadTest1.run // a()<br />
                  2 40.10% 99.98%    3239 300033 test.Test$ThreadTest2.run // b()<br /><br />
              </tt>

              Though this profile again indicates that we spend more execution time in <tt>a()</tt>,
              it still does not indicate that optimizing <tt>b()</tt> does nothing for runtime and that
              after we speed up <tt>a()</tt> by 33% that the program runtime will top off as <tt>b()</tt>
              will become the bottleneck.
              </p>

              <p>
              Now, observe the output of JCoz on the same exact program:

                <i><b>INSERT GRAPH OF OUTPUT ONCE IT'S DONE RUNNING</b></i>

                We demonstrate more results below in the results section.
              </p>

              <h2>Approach</h2>
              <p>
                  The runtime of our profiler generally follows this workflow:
                  <ol>
                      <li>Start the program, and set a breakpoint (a "progress point") at a line in the program that was chosen by the user to measure throughput</li>
                      <li>Wait for a given warmup period to avoid overhead during the initial part of a program where the progress point will not be hit</li>
                      <li>Starting running experiments. An experiment includes the following steps:
                            <ol>
                                <li>Choose a line for speedup randomly among the currently executing threads, as well as a random speedup between 0 - 100%</li>
                                <li>
                                    Every 1ms, send a SIGPROF signal to all user threads, in which they check whether they are on the experiment line. If so,
                                    all other threads are frozen for a period of time dependent on the speedup chosen for this experiment, and the given thread continues
                                    to execute
                                </li>
                                <li>
                                    Throughout the runtime of the experiment, keep track of how many times a given "progress-point" line is hit by all threads. This
                                    progress point line is how we measure how throughput changes when we virtually speed up a line
                                </li>
                                <li>After a certain amount of time, end the experiment and write the metrics to a file buffer</li>
                            </ol>
                      </li>
                      <li>When the program has finished running, flush the file buffer</li>
                  </ol>
                  A program can be run more than once -- the results are simply appended to the given profile output.
              </p>
              <p>
                  In building our tool, we leveraged the <a href="https://docs.oracle.com/javase/8/docs/platform/jvmti/jvmti.html">JVM Tool Interface (JVMTI)</a>
                  to get thread stack traces, listen for when new user threads are created, and existing threads are killed, and map byte code instructions to source file
                  line numbers. Our profiler is written in C++ as a Java Agent, and we target Linux machines.
              </p>
              <p>
                  We ocassionally referenced the <a href="https://github.com/plasma-umass/coz">original COZ implementation</a> 
                  to gain insights into how they implemented certain techniques. In addition, we integrated parts of our code with the
                  <a href="https://github.com/dcapwell/lightweight-java-profiler">lightweight-java-profiler</a> to take advantage of their approach to
                  using the undocumented JVMTI method <tt>AsyncGetCallTrace</tt>. The documented JVMTI method for getting thread stack traces, <tt>GetStackTrace</tt>,
                  waits until a "safe point" before returning the stack trace. This would have killed our profiler, as the JVMTI would have masked opportunities for identifying
                  specific lines where bottlenecks can occur (as we know, parallel bottlenecks can often be relegated to a single line).
              </p>

            <h2>Results</h2>
              <p>
                  We achieved success in two ways. Firstly, we are confident that our profiler is accurate and robust to many different types of parallel programs, as well as
                  sufficiently lightweight so as to accurately reflect program runtimes and speedups. We tested our
                  profiler on a variety of toy examples that exemplify various parallal patterns and found that our profiler returned the expected results for those examples. In addition,
                  we were able to add several optimizations to our profiler that make it robust to scaling out to any number of threads (on a single node), as well as having the option to
                  either measure end-to-end runtime of the program, or measure throughput at a given point in the codebase being profiled. In addition, we were able to use our profiler
                  to find a bottleneck in a well known, mature library: the <a href="http://www.h2database.com/html/main.html">Java H2 Database</a>.
              </p>
                
              <p>
                  The crowning achievement of the Java 
              </p>
              
          
          <!--
              <p>
                  JCoz joins a community of existing Java profilers, including 
                  <a href="http://docs.oracle.com/javase/7/docs/technotes/samples/hprof.html">hprof</a>,
                  <a href="http://jrat.sourceforge.net/">JRat</a>,
                  <a href="https://www.yourkit.com/java/profiler/">YourKit</a>,
                  and a <a href="https://github.com/dcapwell/lightweight-java-profiler">lightweight java profiler</a> by
                  Jeremy Manson and David Capwell.
              </p>
              -->

          <h2>References</h2>
              <p>
                  <ol>
                      <li>C. Curtsinger, E. Berger. COZ: Finding Code that Counts with Causal Profiling. <i>SOSP '15 ACM SIGOPS</i></li>
                  </ol>
              </p>
      </section>
  </body>
</html>
